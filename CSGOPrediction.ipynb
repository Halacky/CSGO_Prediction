{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_ktnmmF7zaZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import lightgbm\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "import lightgbm as lgbm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Здесь были исследования (можно не смотреть)\n"
      ],
      "metadata": {
        "id": "KzIklAvX3Sbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feats = pd.read_csv('/content/players_feats.csv')\n",
        "train = pd.read_csv('/content/train.csv')\n",
        "\n",
        "feats['who_win'] = 0\n",
        "map_names = feats['map_name'].unique()\n",
        "print(map_names)\n",
        "map_dict = {\n",
        "    'Ancient': 0, \n",
        "    'Dust2': 1, \n",
        "    'Mirage': 2,\n",
        "    'Inferno': 3,\n",
        "    'Nuke': 4,\n",
        "    'Overpass':5,\n",
        "    'Vertigo': 6\n",
        "}\n",
        "feats['map_name'] = feats['map_name'].map(map_dict).fillna(7)\n",
        "\n",
        "for match in train.iterrows():\n",
        "  match_info = list(match[1])\n",
        "  team2_id = int(match_info[2])\n",
        "  map_id = int(match_info[0])\n",
        "  who_win = int(train[train['map_id'] == map_id]['who_win'])\n",
        "  if who_win == 1:\n",
        "    feats.loc[(feats['map_id'] == map_id) & (feats['team_id'] == team2_id), 'who_win'] = 1\n",
        "\n",
        "X = feats[np.isfinite(feats).all(1)]\n",
        "y = X['who_win'].values\n",
        "maps_id = X['map_id']\n",
        "teams_id = X['team_id']\n",
        "X1 = X.values #returns a numpy array\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(X1)\n",
        "df = pd.DataFrame(x_scaled)\n",
        "df.columns = feats.columns\n",
        "X_new = df.drop(['who_win','p1_id','p2_id','p3_id','p4_id','p5_id'], axis = 1)\n",
        "X_new['map_id'] = maps_id\n",
        "X_new['team_id'] = teams_id\n",
        "\n",
        "# X_new['map_id'].replace('', np.nan, inplace=True)\n",
        "# X_new.dropna(subset=['map_id'], inplace=True)\n",
        "# y = X_new['who_win'].values\n",
        "# X_new = X_new.drop(['who_win'], axis = 1)\n",
        "print(len(y))\n",
        "print(len(X_new))\n",
        "\n",
        "# X_new.drop(X_new[X_new.map_id.value_counts() == 1].index, inplace=True)\n",
        "\n",
        "# sns.pairplot(data=X_new, diag_kind='kde')\n",
        "# X_new.to_excel('tr1.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvoYDTUXSrE3",
        "outputId": "ddc39006-87c1-47c7-f0c6-ca14176e5d10"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ancient' 'Dust2' 'Mirage' 'Inferno' 'Nuke' 'Overpass' 'Vertigo']\n",
            "1470\n",
            "1470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_new,y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#Create a XGBoost Regressor\n",
        "reg = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n",
        "\n",
        "# reg = RandomForestRegressor()\n",
        "\n",
        "# sc = StandardScaler()\n",
        "# X_train = sc.fit_transform(X_train)\n",
        "# reg = svm.SVR()\n",
        "\n",
        "# # Train the model using the training sets \n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# # Model prediction on train data\n",
        "y_pred = reg.predict(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P07Cf25uk3s4",
        "outputId": "abc5ba5f-bdd3-488d-af0d-f99bab85588b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[03:05:31] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def boost_models(x):\n",
        "    #transforming target variable through quantile transformer\n",
        "    regr_trans = TransformedTargetRegressor(regressor=x, transformer=QuantileTransformer(output_distribution='normal'))\n",
        "    regr_trans.fit(X_train, y_train)\n",
        "    yhat = regr_trans.predict(X_test)\n",
        "    algoname= x.__class__.__name__\n",
        "    return algoname, round(metrics.r2_score(y_test, yhat),3), round(metrics.mean_absolute_error(y_test, yhat),2), round(np.sqrt(metrics.mean_squared_error(y_test, yhat)),2), round(metrics.roc_auc_score(y_test, yhat),2)\n",
        "\n",
        "algo=[GradientBoostingRegressor(), lgbm.LGBMRegressor(), xgb.XGBRFRegressor()]\n",
        "score=[]\n",
        "for a in algo:\n",
        "    score.append(boost_models(a))\n",
        "\n",
        "pd.DataFrame(score, columns=['Model', 'Score', 'MAE', 'RMSE', 'ROC-AUC'])"
      ],
      "metadata": {
        "id": "x6_ZkgHe5wlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_estimators': [100, 80, 60, 55, 51, 45],  \n",
        "              'max_depth': [7, 8],\n",
        "              'reg_lambda' :[0.26, 0.25, 0.2]\n",
        "             }\n",
        "                \n",
        "grid = GridSearchCV(xgb.XGBRFRegressor(), param_grid, refit = True, verbose = 3, n_jobs=-1) #\n",
        "regr_trans = TransformedTargetRegressor(regressor=grid, transformer=QuantileTransformer(output_distribution='normal'))\n",
        "\n",
        "# fitting the model for grid search \n",
        "grid_result=regr_trans.fit(X_train, y_train)\n",
        "best_params=grid_result.regressor_.best_params_\n",
        "print(best_params)\n",
        "\n",
        "#using best params to create and fit model\n",
        "best_model = xgb.XGBRFRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"], reg_lambda=best_params[\"reg_lambda\"])\n",
        "regr_trans = TransformedTargetRegressor(regressor=best_model, transformer=QuantileTransformer(output_distribution='normal'))\n",
        "regr_trans.fit(X_train, y_train)\n",
        "yhat = regr_trans.predict(X_test)\n",
        "\n",
        "#evaluate metrics\n",
        "metrics.r2_score(y_test, yhat), metrics.mean_absolute_error(y_test, yhat), np.sqrt(metrics.mean_squared_error(y_test, yhat)), metrics.roc_auc_score(y_test, yhat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biHRlX6s8EHA",
        "outputId": "164f7f3f-0a79-45ab-e700-c6b84e1bd3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "[12:25:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "{'max_depth': 7, 'n_estimators': 100, 'reg_lambda': 0.2}\n",
            "[12:25:15] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.2930555555555556,\n",
              " 0.19387755102040816,\n",
              " 0.44031528592635544,\n",
              " 0.5081018518518517)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "acc_ROC_AUC_train= metrics.roc_auc_score(y_train, y_pred)\n",
        "acc_R2_train= metrics.r2_score(y_train, y_pred)\n",
        "acc_AR2_train= 1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n",
        "acc_MAE_train= metrics.mean_absolute_error(y_train, y_pred)\n",
        "acc_MSE_train= metrics.mean_squared_error(y_train, y_pred)\n",
        "acc_RMSE_train= np.sqrt(metrics.mean_squared_error(y_train, y_pred))\n",
        "\n",
        "print('R^2:',acc_R2_train)\n",
        "print('Adjusted R^2:', acc_AR2_train)\n",
        "print('MAE:', acc_MAE_train)\n",
        "print('MSE:', acc_MSE_train)\n",
        "print('RMSE:', acc_RMSE_train)\n",
        "print(\"ROC-AUC:\", acc_ROC_AUC_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh6SzcTdlLt0",
        "outputId": "b6df7903-cc5a-4b56-90bd-81a304489136"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2: 0.9999999956684433\n",
            "Adjusted R^2: 0.9999999951573938\n",
            "MAE: 2.6306124771533368e-05\n",
            "MSE: 7.621667469484052e-10\n",
            "RMSE: 2.7607367620771185e-05\n",
            "ROC-AUC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/test.csv')\n",
        "y_test_pred = reg.predict(test)\n",
        "\n",
        "print(X_test)\n",
        "\n",
        "X_test['aaa'] = y_test_pred\n",
        "\n",
        "# X_test.to_excel('aaaTest.xlsx')"
      ],
      "metadata": {
        "id": "lA9Pzd1ilMk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "acc_ROC_AUC_train= metrics.roc_auc_score(y_test, y_test_pred)\n",
        "acc_R2_train= metrics.r2_score(y_test, y_test_pred)\n",
        "acc_AR2_train= 1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)/(len(y_test)-X_train.shape[1]-1)\n",
        "acc_MAE_train= metrics.mean_absolute_error(y_test, y_test_pred)\n",
        "acc_MSE_train= metrics.mean_squared_error(y_test, y_test_pred)\n",
        "acc_RMSE_train= np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "print('R^2:',acc_R2_train)\n",
        "print('Adjusted R^2:', acc_AR2_train)\n",
        "print('MAE:', acc_MAE_train)\n",
        "print('MSE:', acc_MSE_train)\n",
        "print('RMSE:', acc_RMSE_train)\n",
        "print(\"ROC-AUC:\", acc_ROC_AUC_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF3nDTNpwfqd",
        "outputId": "19f8c4f3-8299-4ddd-c149-5ba3b69c6246"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2: -0.10516416367574166\n",
            "Adjusted R^2: -0.904782940923484\n",
            "MAE: 0.3326640230457799\n",
            "MSE: 0.16570558055946144\n",
            "RMSE: 0.40706950335226716\n",
            "ROC-AUC: 0.5722993827160494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Результат получен здесь"
      ],
      "metadata": {
        "id": "p6d7KfQrfnML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Подгружаем данные\n",
        "feats = pd.read_csv('/content/players_feats.csv') \n",
        "train = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Приводим к цифрому виду названия карт\n",
        "map_dict = {\n",
        "    'Ancient': 0, \n",
        "    'Dust2': 1, \n",
        "    'Mirage': 2,\n",
        "    'Inferno': 3,\n",
        "    'Nuke': 4,\n",
        "    'Overpass':5,\n",
        "    'Vertigo': 6\n",
        "}\n",
        "feats['map_name'] = feats['map_name'].map(map_dict).fillna(7)\n",
        "\n",
        "# Пропущенные значения, заменяем на 0, возможно, имеет смысл вообще удалить строки с нулевыми значениями или зменить например на среднее \n",
        "feats.fillna(0, inplace = True)\n",
        "\n",
        "# Здесь будем хранить значения вида Комнда/карта: уровнь скилла на этой карте\n",
        "grp = {}\n",
        "\n",
        "# Добавляем столбец с меткой результата матча \n",
        "feats['who_win'] = 0\n",
        "\n",
        "# Заполняем стоблец с результатами матча, согласно данным из train.csv\n",
        "for match in train.iterrows():\n",
        "  match_info = list(match[1])\n",
        "  team2_id = int(match_info[2])\n",
        "  map_id = int(match_info[0])\n",
        "  who_win = int(train[train['map_id'] == map_id]['who_win'])\n",
        "  if who_win == 1:\n",
        "    feats.loc[(feats['map_id'] == map_id) & (feats['team_id'] == team2_id), 'who_win'] = 1\n",
        "\n",
        "# Сохраняем значения столбцов, которые мы не хотим нормировать\n",
        "team_id = feats['team_id']\n",
        "map_name = feats['map_name']\n",
        "\n",
        "# Нормируем значения (значения варьируются от 0 до 1)\n",
        "X = feats.values \n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scaled = min_max_scaler.fit_transform(X)\n",
        "df = pd.DataFrame(x_scaled)\n",
        "\n",
        "df.columns = feats.columns\n",
        "df['team_id'] = team_id\n",
        "df['map_name'] = map_name\n",
        "\n",
        "df.drop(['p1_id','p2_id','p3_id','p4_id','p5_id'], axis = 1, inplace=True)\n",
        "# Функция на вход получает алгоритм Регресси и данные, на которых обучается и делает предсказания \n",
        "# Предсказания округляем\n",
        "def boost_models(x,tmp_X, tmp_y,X_test):\n",
        "    regr_trans = TransformedTargetRegressor(regressor=x, transformer=QuantileTransformer(output_distribution='normal'))\n",
        "    regr_trans.fit(tmp_X, tmp_y)\n",
        "    yhat = regr_trans.predict(tmp_X)\n",
        "    return yhat.mean()\n",
        "\n",
        "# Усредняем все значения, тем самым получаем средний показатель скилла конкретной команды, на конкретной карте \n",
        "def get_score(tmp_X, tmp_y,X_test=[]):\n",
        "  algo=[GradientBoostingRegressor(), lgbm.LGBMRegressor(), xgb.XGBRFRegressor()]\n",
        "  score = []\n",
        "  for a in algo:\n",
        "    score.append(boost_models(a,tmp_X, tmp_y,X_test))\n",
        "  return np.mean(score)\n",
        "\n",
        "# Получаем значения конкретной команды на определенной карте \n",
        "for feat in df.iterrows():\n",
        "  feat = list(feat[1])\n",
        "  team_id = int(feat[-4])\n",
        "  map_name = int(feat[-3])\n",
        "\n",
        "  comb_id = str(team_id) + \"/\" + str(map_name)\n",
        "  tmp_X = feats[(feats['team_id'] == team_id) & (feats['map_name'] == map_name)]\n",
        "  tmp_y = tmp_X[\"who_win\"]\n",
        "  tmp_X.drop(['who_win'], axis = 1, inplace=True)\n",
        "  print(tmp_X.shape)\n",
        "  # Здесь я использовал два варианта: \n",
        "  # 1) с разбиением на тренировачный и тестовый (тогда приходится отбрасывать те значения, где команда сыграла 1 или 2 игры на определенной карте)\n",
        "  # 2) Использование всех данных в качестве определения скила (есть риск переобучиться)\n",
        "  if tmp_X.shape[0] < 2:\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(tmp_X, tmp_y, random_state = 42)\n",
        "    grp[comb_id] = get_score(tmp_X, tmp_y)\n",
        "    continue\n",
        "  else:\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(tmp_X, tmp_y, test_size = 0.2, random_state = 42)\n",
        "    # grp[comb_id] = get_score(X_train, y_train, X_test)\n",
        "    grp[comb_id] = get_score(tmp_X, tmp_y)\n",
        "  \n",
        "print(grp)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b5Bo9x_3fpxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "predicts = []\n",
        "\n",
        "for game in test.iterrows():\n",
        "  game_info = list(game[1])\n",
        "  team1_id = game_info[2]\n",
        "  team2_id = game_info[3]\n",
        "  map_name = map_dict[game_info[4]]\n",
        "  team1_skill = grp[str(team1_id)+\"/\"+str(map_name)]\n",
        "  team2_skill = grp[str(team2_id)+\"/\"+str(map_name)]\n",
        "  if team1_skill > team2_skill:\n",
        "    print(f\"T1({team1_id}) = {team1_skill}, T2({team2_id}) = {team2_skill} / {game_info[4]}. Predict = 0\")\n",
        "    predicts.append(0)\n",
        "  elif team1_skill < team2_skill:\n",
        "    print(f\"T1({team1_id}) = {team1_skill}, T2({team2_id}) = {team2_skill} / {game_info[4]}. Predict = 1\")\n",
        "    predicts.append(1)\n",
        "  else:\n",
        "    # Если значения скилла равны, я выставляю 0, но надо вводить какие-то показатели на основе которых будет решаться что делать в такой ситуации\n",
        "    print(f\"T1({team1_id}) = {team1_skill}, T2({team2_id}) = {team2_skill} / {game_info[4]}. Predict = хм\")\n",
        "    predicts.append(0)\n",
        "\n",
        "test['who_win'] = predicts\n",
        "\n",
        "test.to_csv(\"predict.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nCZVoRjE7Px",
        "outputId": "2947e223-abe1-4ba6-a99f-76006064ebc4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T1(5973) = 0.0, T2(5752) = 0.08838822974429857 / Dust2. Predict = 1\n",
            "T1(5973) = 0.18181818452748386, T2(5752) = 0.49999999999999994 / Vertigo. Predict = 1\n",
            "T1(8297) = 0.03450023673042587, T2(7020) = 0.49999999999999994 / Nuke. Predict = 1\n",
            "T1(8297) = 0.12121212301832257, T2(7020) = 0.0 / Mirage. Predict = 0\n",
            "T1(8297) = 0.22222222553359136, T2(7020) = 0.8255547838659324 / Overpass. Predict = 1\n",
            "T1(4494) = 0.12005530508809077, T2(4411) = 0.200000003973643 / Vertigo. Predict = 1\n",
            "T1(4494) = 0.24242424603664514, T2(4411) = 0.050403142234286914 / Inferno. Predict = 0\n",
            "T1(4494) = 0.200000003973643, T2(4411) = 0.0463803037769363 / Mirage. Predict = 0\n",
            "T1(4608) = 0.09694789158992279, T2(7718) = 0.26666666865348815 / Inferno. Predict = 1\n",
            "T1(4608) = 0.12698412816675883, T2(7718) = 0.0 / Mirage. Predict = 0\n",
            "T1(4608) = 0.07062924216357526, T2(7718) = 0.0 / Overpass. Predict = 0\n",
            "T1(6665) = 0.17777778241369457, T2(6667) = 0.17777778241369457 / Overpass. Predict = хм\n",
            "T1(6665) = 0.0463803037769363, T2(6667) = 0.200000003973643 / Inferno. Predict = 1\n",
            "T1(5973) = 0.10256410447450785, T2(8297) = 0.09998126858141354 / Ancient. Predict = 0\n",
            "T1(5973) = 0.0760907918806367, T2(8297) = 0.12121212301832257 / Mirage. Predict = 1\n",
            "T1(4494) = 0.200000003973643, T2(6665) = 0.20833333333333334 / Mirage. Predict = 1\n",
            "T1(4494) = 0.20833333333333334, T2(6665) = 0.23809524093355453 / Nuke. Predict = 1\n",
            "T1(4494) = 0.16666666666666666, T2(6665) = 0.0 / Ancient. Predict = 0\n",
            "T1(7718) = 0.22222222553359136, T2(5973) = 0.10256410447450785 / Ancient. Predict = 0\n",
            "T1(7718) = 0.26666666865348815, T2(5973) = 0.0 / Inferno. Predict = 0\n",
            "T1(7718) = 0.12330902330530473, T2(5973) = 0.18181818452748386 / Vertigo. Predict = 1\n",
            "T1(4608) = 0.22222222553359136, T2(6665) = 0.23809524093355453 / Nuke. Predict = 1\n",
            "T1(4608) = 0.12698412816675883, T2(6665) = 0.20833333333333334 / Mirage. Predict = 1\n",
            "T1(6667) = 0.22222222553359136, T2(7718) = 0.22222222553359136 / Ancient. Predict = хм\n",
            "T1(6667) = 0.31111111177338496, T2(7718) = 0.0 / Nuke. Predict = 0\n",
            "T1(4608) = 0.09694789158992279, T2(6667) = 0.200000003973643 / Inferno. Predict = 1\n",
            "T1(4608) = 0.07062924216357526, T2(6667) = 0.17777778241369457 / Overpass. Predict = 1\n",
            "T1(4608) = 0.07372495935315816, T2(6667) = 0.22222222553359136 / Ancient. Predict = 1\n",
            "T1(4608) = 0.12698412816675883, T2(6667) = 0.17777778241369457 / Mirage. Predict = 1\n",
            "T1(4608) = 0.22222222553359136, T2(6667) = 0.31111111177338496 / Nuke. Predict = 1\n"
          ]
        }
      ]
    }
  ]
}